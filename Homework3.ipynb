{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xJsYv_-x_tx"
      },
      "source": [
        "### Homework 3 logbook\r\n",
        "This time I used two encoders, one for image analysis and another for sequence analysis. Since this task can be seen as a classification task I used two dense layers on the top of the net to sum up the features extracted from the two encoders. This net is not able to answer location based questions since it only encodes the features without decoding so it may have troubles to locate a feature. Attention mechanism may increase the overall accuracy. In conclusion this net can give almost +0.3 accuracy points with respect to the baseline 0.3 (since 30% of the answer in the training are \"yes\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiLwi7HcT6Zm"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJqHEF3eT6Zn"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for random operations. \n",
        "# This let our experiments to be reproducible. \n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Get current working directory\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5NrvC17nE_Y"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2mD2CM9nttF"
      },
      "source": [
        "# Use Kaggle token to download dataset on drive\r\n",
        "import os\r\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxBHcug9nxiE"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNN8APZkoQpb"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBXiyUVCn1Gi"
      },
      "source": [
        "!kaggle competitions download -c anndl-2020-vqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFlbqOjpppQw"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAADhpmjqX_n"
      },
      "source": [
        "if not os.path.exists(\"/content/VQA_Dataset\"):\r\n",
        "  !unzip \"anndl-2020-vqa.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0g91OJ-ruuF"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V42N6SDxr16J"
      },
      "source": [
        "%ls VQA_Dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeszjH0iT6Zo"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TObrO0GxVS0"
      },
      "source": [
        "from PIL import Image\r\n",
        "import json\r\n",
        "\r\n",
        "with open('VQA_Dataset/train_questions_annotations.json') as f:\r\n",
        "  ds_dict = json.load(f)\r\n",
        "\r\n",
        "ds_list = list(ds_dict.values())\r\n",
        "qst, img, ans = ds_list[0].values()\r\n",
        "qst\r\n",
        "ans\r\n",
        "img = Image.open(\"VQA_Dataset/Images/\"+img+\".png\")\r\n",
        "img_arr = np.array(img)[...,:3]\r\n",
        "img_arr.shape\r\n",
        "\r\n",
        "Image.fromarray(img_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75sNY9KBT6Zo"
      },
      "source": [
        "# Tokenization\n",
        "## Converts words to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqOWPnu3SjnR"
      },
      "source": [
        "# Prepare dataset\r\n",
        "# ---------------\r\n",
        "\r\n",
        "qst = []\r\n",
        "img = []\r\n",
        "ans = []\r\n",
        "\r\n",
        "for item in ds_list:\r\n",
        "  q, i, a = item.values()\r\n",
        "\r\n",
        "  qst.append(q)\r\n",
        "  img.append(i)\r\n",
        "  ans.append(a)\r\n",
        "\r\n",
        "print('Number of sentences:', len(qst))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjwAwhW-dm2Z"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(qst)\r\n",
        "tokenized = tokenizer.texts_to_sequences(qst)\r\n",
        "\r\n",
        "wtoi = tokenizer.word_index\r\n",
        "print('Total question words:', len(wtoi))\r\n",
        "\r\n",
        "max_qst_length = max(len(q) for q in tokenized)\r\n",
        "print('Max question sentence length:', max_qst_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNJss7nAO88o"
      },
      "source": [
        "labels_dict = {\r\n",
        "        '0': 0,\r\n",
        "        '1': 1,\r\n",
        "        '2': 2,\r\n",
        "        '3': 3,\r\n",
        "        '4': 4,\r\n",
        "        '5': 5,\r\n",
        "        'apple': 6,\r\n",
        "        'baseball': 7,\r\n",
        "        'bench': 8,\r\n",
        "        'bike': 9,\r\n",
        "        'bird': 10,\r\n",
        "        'black': 11,\r\n",
        "        'blanket': 12,\r\n",
        "        'blue': 13,\r\n",
        "        'bone': 14,\r\n",
        "        'book': 15,\r\n",
        "        'boy': 16,\r\n",
        "        'brown': 17,\r\n",
        "        'cat': 18,\r\n",
        "        'chair': 19,\r\n",
        "        'couch': 20,\r\n",
        "        'dog': 21,\r\n",
        "        'floor': 22,\r\n",
        "        'food': 23,\r\n",
        "        'football': 24,\r\n",
        "        'girl': 25,\r\n",
        "        'grass': 26,\r\n",
        "        'gray': 27,\r\n",
        "        'green': 28,\r\n",
        "        'left': 29,\r\n",
        "        'log': 30,\r\n",
        "        'man': 31,\r\n",
        "        'monkey bars': 32,\r\n",
        "        'no': 33,\r\n",
        "        'nothing': 34,\r\n",
        "        'orange': 35,\r\n",
        "        'pie': 36,\r\n",
        "        'plant': 37,\r\n",
        "        'playing': 38,\r\n",
        "        'red': 39,\r\n",
        "        'right': 40,\r\n",
        "        'rug': 41,\r\n",
        "        'sandbox': 42,\r\n",
        "        'sitting': 43,\r\n",
        "        'sleeping': 44,\r\n",
        "        'soccer': 45,\r\n",
        "        'squirrel': 46,\r\n",
        "        'standing': 47,\r\n",
        "        'stool': 48,\r\n",
        "        'sunny': 49,\r\n",
        "        'table': 50,\r\n",
        "        'tree': 51,\r\n",
        "        'watermelon': 52,\r\n",
        "        'white': 53,\r\n",
        "        'wine': 54,\r\n",
        "        'woman': 55,\r\n",
        "        'yellow': 56,\r\n",
        "        'yes': 57\r\n",
        "}\r\n",
        "\r\n",
        "data = np.array([labels_dict[a] for a in ans])\r\n",
        "shape = (data.size, len(labels_dict))\r\n",
        "one_hot = np.zeros(shape)\r\n",
        "\r\n",
        "rows = np.arange(data.size)\r\n",
        "one_hot[rows, data] = 1\r\n",
        "print(one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le0uh2IYnOd5"
      },
      "source": [
        "print(\"FREQ\\t\\t\\tLABEL\")\r\n",
        "labels = np.array(list(labels_dict.keys()))\r\n",
        "freqs = np.array(one_hot.sum(axis=0) / one_hot.sum())\r\n",
        "\r\n",
        "stats = np.array(list(zip(freqs, labels)))\r\n",
        "\r\n",
        "stats = stats[stats[:,0].argsort()]\r\n",
        "\r\n",
        "for stat in stats[::-1]:\r\n",
        "  print(\"{}\\t{}\".format(stat[0], stat[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMHmTPzdT6Zo"
      },
      "source": [
        "# Padding sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsAMc718T6Zp"
      },
      "source": [
        "encoder_inputs = pad_sequences(tokenized, maxlen=max_qst_length)\n",
        "\n",
        "print(\"Encoder inputs shape:\", encoder_inputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G58-ZmdtT6Zp"
      },
      "source": [
        "encoder_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsieZk4aKhm6"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, dataset_dir, which_subset, qst, img, ans, out_shape = [256, 256], preprocessing_function = None, val_split = 0):\n",
        "\n",
        "    size = int(len(qst)*(1-val_split))\n",
        "\n",
        "    self.dataset_dir = dataset_dir\n",
        "    self.which_subset = which_subset\n",
        "    if which_subset == 'test':\n",
        "      self.qst = qst\n",
        "      self.img = img\n",
        "      self.ans = ans\n",
        "    else:\n",
        "      self.qst = qst[:size] if which_subset == 'training' else qst[size:]\n",
        "      self.img = img[:size] if which_subset == 'training' else img[size:]\n",
        "      self.ans = ans[:size] if which_subset == 'training' else ans[size:]\n",
        "    self.out_shape = out_shape\n",
        "    self.preprocessing_function = preprocessing_function\n",
        "    \n",
        "    print(which_subset + ' ' + str(len(self.qst)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.qst)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Read Image\n",
        "    curr_filename = self.img[index]\n",
        "    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.png'))\n",
        "    img = img.resize(self.out_shape)\n",
        "    img_arr = np.array(img)[...,:3]\n",
        "\n",
        "    if self.preprocessing_function is not None:\n",
        "      img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "    if self.which_subset == 'test':\n",
        "      return {\"input_1\": self.qst[index], \"input_2\": img_arr}\n",
        "    else:\n",
        "      return {\"input_1\": self.qst[index], \"input_2\": img_arr}, self.ans[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyrdiIh_PWjB"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50  import preprocess_input \n",
        "\n",
        "img_h = 400\n",
        "img_w = 700\n",
        "\n",
        "dataset = CustomDataset('VQA_Dataset', 'training', encoder_inputs, img, one_hot, [img_w, img_h], preprocess_input)\n",
        "dataset_valid = CustomDataset('VQA_Dataset', 'validation', encoder_inputs, img, one_hot, [img_w, img_h], preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usz5SKPeQrOE"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
        "                                               output_types=({\"input_1\": tf.float32, \"input_2\": tf.float32}, tf.int32),\n",
        "                                               output_shapes=({\"input_1\": [max_qst_length], \"input_2\": [img_h, img_w, 3]}, [labels.size]))\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
        "                                               output_types=({\"input_1\": tf.float32, \"input_2\": tf.float32}, tf.int32),\n",
        "                                               output_shapes=({\"input_1\": [max_qst_length], \"input_2\": [img_h, img_w, 3]}, [labels.size]))\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wa_q1YQab-H"
      },
      "source": [
        "# Translation utils\r\n",
        "itow = {v:k for k, v in wtoi.items()}\r\n",
        "\r\n",
        "def translate(input_sentence):\r\n",
        "\r\n",
        "  output_sentence = []\r\n",
        "  for word_id in input_sentence:\r\n",
        "\r\n",
        "    if word_id > 0:\r\n",
        "        word = itow[word_id]\r\n",
        "        output_sentence.append(word)\r\n",
        "    \r\n",
        "  return ' '.join(output_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDO_m9qAaEpF"
      },
      "source": [
        "# Let's test data generator\r\n",
        "# -------------------------\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "iterator = iter(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhI8nUdmK8K-"
      },
      "source": [
        "l, a = next(iterator)\r\n",
        "q, i = l.values()\r\n",
        "\r\n",
        "fig, ax = plt.subplots(1, 1)\r\n",
        "\r\n",
        "translate(q[0].numpy())\r\n",
        "print(labels[np.argmax(a[0])])\r\n",
        "img_arr = i[0].numpy()\r\n",
        "ax.imshow(np.uint8(img_arr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyx28iuHT6Zp"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmutbKToiVgx"
      },
      "source": [
        "base_model = tf.keras.applications.ResNet50(input_shape=[img_h, img_w, 3], include_top=False)\r\n",
        "\r\n",
        "for layer in base_model.layers:\r\n",
        "  base_model.trainable = False\r\n",
        "\r\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IC7MPwGT6Zp"
      },
      "source": [
        "# Build Encoder-Decoder Model\n",
        "# ---------------------------\n",
        "\n",
        "EMBEDDING_SIZE = 32\n",
        "\n",
        "# ENCODER SEQ\n",
        "# -----------\n",
        "\n",
        "encoder_input = tf.keras.Input(shape=[max_qst_length], name=\"input_1\")\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(len(wtoi)+1, EMBEDDING_SIZE, input_length=max_qst_length, mask_zero=True)\n",
        "encoder_embedding_out = encoder_embedding_layer(encoder_input)\n",
        "encoder = tf.keras.layers.LSTM(units=256, return_state=True)\n",
        "\n",
        "encoder_output, h, c = encoder(encoder_embedding_out)\n",
        "\n",
        "# ENCODER IMG\n",
        "# -----------\n",
        "\n",
        "decoder_input = tf.keras.Input(shape=[img_h, img_w, 3], name=\"input_2\")\n",
        "x = base_model(decoder_input)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=128,\n",
        "                            kernel_size=(3, 3),\n",
        "                            strides=(1, 1),\n",
        "                            padding='same',\n",
        "                            activation='relu')(x)\n",
        "x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(tf.concat([x, h], -1))\n",
        "decoder_dense = tf.keras.layers.Dense(len(labels), activation='softmax')\n",
        "decoder = decoder_dense(x)\n",
        "\n",
        "# MODEL\n",
        "model = tf.keras.Model([encoder_input, decoder_input], decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBGKVJNYT6Zp"
      },
      "source": [
        "model.summary(line_length=200)\n",
        "# model.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Lw7k0pT6Zp"
      },
      "source": [
        "# Prepare model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ihm4kvT6Zp"
      },
      "source": [
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "\n",
        "metrics = ['accuracy']\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlbGCZQUT6Zp"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "exps_dir = os.path.join('/content/gdrive/My Drive/Homework3', 'experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "exp_name = 'exp'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "# ----------------\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=False)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# # Early Stopping\n",
        "# # --------------\n",
        "# early_stop = False\n",
        "# if early_stop:\n",
        "#     es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
        "#     callbacks.append(es_callback)\n",
        "\n",
        "# # ---------------------------------\n",
        "\n",
        "model.fit(x=train_dataset,\n",
        "          validation_data=valid_dataset,\n",
        "          steps_per_epoch=len(dataset)//BATCH_SIZE,\n",
        "          validation_steps=len(dataset_valid)//BATCH_SIZE,\n",
        "          epochs=1,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44SnJfWwx0fe"
      },
      "source": [
        "from PIL import Image\r\n",
        "import json\r\n",
        "\r\n",
        "with open('VQA_Dataset/test_questions.json') as f:\r\n",
        "  dst_dict = json.load(f)\r\n",
        "\r\n",
        "dst_list = list(dst_dict.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqXS6zK7aQcW"
      },
      "source": [
        "# Prepare testset\r\n",
        "# ---------------\r\n",
        "\r\n",
        "key = list(dst_dict.keys())\r\n",
        "qst = []\r\n",
        "img = []\r\n",
        "\r\n",
        "for item in dst_list:\r\n",
        "    \r\n",
        "  q, i = item.values()\r\n",
        "\r\n",
        "  qst.append(q)\r\n",
        "  img.append(i)\r\n",
        "\r\n",
        "print('Number of sentences:', len(qst))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOCJ_RuaQca"
      },
      "source": [
        "tokenized = tokenizer.texts_to_sequences(qst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i33yqUxzaQcb"
      },
      "source": [
        "encoder_inputs = pad_sequences(tokenized, maxlen=max_qst_length)\n",
        "\n",
        "print(\"Encoder inputs shape:\", encoder_inputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PKvQMUYaQcc"
      },
      "source": [
        "encoder_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee0M5VByaQcc"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input \n",
        "\n",
        "dataset_test = CustomDataset('VQA_Dataset', 'test', encoder_inputs, img, None, [img_w, img_h], preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awCOfdm6aQcc"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_generator(lambda: dataset_test,\n",
        "                                               output_types=({\"input_1\": tf.float32, \"input_2\": tf.float32}),\n",
        "                                               output_shapes=({\"input_1\": [max_qst_length], \"input_2\": [img_h, img_w, 3]}))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC-vyB4NF5hE"
      },
      "source": [
        "ans = model.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUVpvIJ_bg-b"
      },
      "source": [
        "# Let's visualize results\r\n",
        "# ----------------------\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "idx = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3p3EZWvbg-j"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(16, 32))\r\n",
        "\r\n",
        "k = key[idx]\r\n",
        "q = qst[idx]\r\n",
        "i = img[idx]\r\n",
        "a = ans[idx]\r\n",
        "\r\n",
        "print(q, end='')\r\n",
        "\r\n",
        "# Original Image\r\n",
        "i2 = Image.open(os.path.join('VQA_Dataset', 'Images', i + '.png'))\r\n",
        "i2 = i2.resize([img_w, img_h])\r\n",
        "i2_arr = np.array(i2)[...,:3]\r\n",
        "idx += 1;\r\n",
        "\r\n",
        "labels = np.array(list(labels_dict.keys()))\r\n",
        "print(labels[np.argmax(a)])\r\n",
        "ax.imshow(np.uint8(i2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deuP2btjHXbA"
      },
      "source": [
        "import os\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "def create_csv(results, results_dir='./'):\r\n",
        "\r\n",
        "    csv_fname = 'results_'\r\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\r\n",
        "\r\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\r\n",
        "\r\n",
        "        f.write('Id,Category\\n')\r\n",
        "\r\n",
        "        for key, value in results.items():\r\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2fw5C57Kdtt"
      },
      "source": [
        "submission = {}\r\n",
        "for idx in range(len(ans)):\r\n",
        "  submission[key[idx]] = np.argmax(ans[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJKLftQjK-5d"
      },
      "source": [
        "create_csv(submission, '/content/gdrive/My Drive/Homework3') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}